# NBA Shot Predictor

## Data Description
We used a data set that consisted of around 120,000 shots taken in the 2014-2015 NBA season. The target variable is field goal made and we are trying to predict whether a shot goes in or not based on a collection of features about each shot. Some of the features include the shooter’s name, game ID, the distance the shot was taken, the distance of the closest defender, and the number of dribbles the shooter took before shooting. We decided to add certain season-long features to the data set to give more insight into a particular shot’s likelihood, such as the shooter’s field goal percentage (FG%) and the defender’s defensive box plus minus (DBPM). These player statistics tell us how good a shooter is at putting the ball in the hoop and how well the closest defender defends which are important in assessing shot quality. 

One of the first things we did to understand the distributions of our set of features was to plot histograms. We used the histogram data to confirm some of our previous basketball knowledge and determine how to preprocess the data before modeling. Shown below are distributions for the time left in the shot clock when the shot was taken, the closest defender distance, the amount of time the ball was in the shooter’s hand before the shot, and the distance the shot was taken. 

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/ShotClockDistribution.png "Shot clock distribution")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/ShotDistanceDistribution.png "Shot distance distribution")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/ClosestDefenderDistanceDistribution.png "Closest defender distance distribution")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/TouchTimeDistribution.png "Touch time distribution")

In every NBA possession, the team with the basketball has 24 seconds to get a shot up before forcing a turnover to the other team. The histogram shows that shots are nearly normally distributed, with the average shot taken at about 12 seconds left in the shot clock. We presume that shots taken early in the shot clock are due to quick plays. These generally happen if a team is coming from a timeout and ran a successful inbounds play or if they stole the ball while on defense, resulting in a fast break. We suspect that shots taken late in the shot clock are due to longer plays or if a play went dead and the players had to improvise. From the plot, we can reasonably conclude that most shots are taken after some preparation time but not long enough to significantly increase the probability of making a mistake. 

We believed that one of the two most important factors in determining shot quality is how far the shooter was when he shot the ball. We expected to see a higher percentage of made shots the closer the shooter is to the hoop. As evidenced by the plot, the shot distance distribution typically has less shots at further distances with one caveat. The distribution is bimodal because at a shot distance of 23.5 feet or more, the shot is worth 3 points as opposed to 2.  As claimed earlier, this trend started becoming more extreme due to recent NBA data analytics. Teams are realizing that incorporating more three point shooting into their offense will spread out the floor for a wider range of plays and increase the average points earned per possession. Based on the distribution, we decided to run all our models on 2 point shots and 3 point shots separately. 

We hypothesized that the second most important factor in determining shot quality is how close the nearest defender was.The closest defender distance distribution is right-skewed with a median distance of 3.7 feet and a mean distance of 4.1 feet. Defenders stay close enough to the ball-holder to contest a potential shot but not too close because the player can run past the defender. A good offensive team can typically get players uncontested shots through fast breaks or well-designed plays. We suspect that if the closest defender to a particular shot is far, the shot will have a high chance of going in. 

Finally, we included information about how long the shooter held the ball in his hands. Most players on an NBA team are not ball dominant; rather, the playmakers of the team have the longest touch time per possession and other players shoot the ball if they receive a pass and have an open look. The distribution shows a vast majority of shooters only hold the ball for a few seconds before shooting. We later scraped data that included percentage of a player’s shots that are assisted. We suspect that shooters with a low touch time and a high percentage of assisted shots should have a high chance of making a basket. 

## Data Preprocessing
Our data set had a variety of errors, missing values, extraneous features, and collinearities. For instance, some shots had no shot clock present, some three point shots were classified as two point shots, etc. When we later incorporated distance based FG%, we had to account for collinearities that exist with overall FG%. We had to use our prior knowledge and basketball reference websites to account for all known issues and throw away data with errors or unknowns. 

We first determined which features were not important in determining shot quality. We discarded information regarding which teams played, final point differential of the game, and if the game played resulted in a win for the shooter. These characteristics describe more about the overall quality of the game and the teams that played but give very little information about the shot. Though it is true that players play much better against weaker defenses, that information is encoded within the closest defender’s statistics. Next, we had to dummy code our categorical variables: quarter and game location. Every NBA game has four quarters with the possibility of extra time if the game ends in a tie at regulation. In addition, teams tend to have better winning records at home than they do as visitors. We wanted to see if the better winning record at home also corresponded to better shooting accuracy. 

We made significant efforts to clean up some components of our messy data. We had several data points that did not have available shot clock data. However, we realized that in some cases, this was intentional. When there are less than 24 seconds left in the quarter, the shot clock is turned off. To account for this, we just replaced the missing shot clock value with the value of the game clock. If the game clock had a value greater than 24 seconds and there was still a missing shot clock, we discarded the data. We observed that certain shots taken beyond 23.5 feet were misclassified as 2-point shots so we correctly classified them as 3-point shots. Finally, when scraping for player statistics from official NBA websites, we realized that our dataset had players with misspelled names. Nevertheless, our knowledge allowed us to manually correct those names and proceed with analysis. 

Along with the errors in data recording, we found some collinearities between some our features. Shown below is a scatter plot between two of our features, number of dribbles and touch time.

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/DribblesvsTouchTime.png "Dribbles vs touch time")

There are a few notable characteristics of this plot. There’s a linear trend between the two features with an R^2 value of 0.86. As we increase the average touch time, the number of dribbles also tends to increase. For some shooters who took zero dribbles before shooting, there were negative touch time values. Because it does not make sense to hold the ball for less than zero seconds, we found the median touch time for all the zero dribble values and replaced the negative touch times with it. In addition, touch time is a continuous variable which gives us more precise information than dribbles about how long the ball was in the shooter’s hand before the shot. We decided to use only touch time because number of dribbles did not give additional information about the quality of the shot. 

After running some initial models, we chose to incorporate FG% by distance. To avoid collinearities with overall FG%, we replaced overall FG% with our new feature. This is because a shooter’s accuracy from certain locations on the field can differ significantly from other locations. For example, taller players tend to have high values for FG% because most of their shots are assisted and taken from a small distance from the hoop. In general, the same players do not shoot the ball nearly as well from longer distances so we hoped having information based on distance would increase our model accuracy. 

Through preprocessing, we realized we had to put significant work into getting the data in a suitable format. We also had to use intuition and preliminary analysis to find out which features were applicable to the problem and what more we could incorporate from open sources.

## Modeling Choices
We wanted to use a variety of classification models on our data set to see which models performed strongly. After some initial evaluation of the models that we learned in class, we decided to use Support Vector Machines (SVM), random forests, Naïve Bayes, logistic regression, gradient boosted decision trees, and XGBoost to classify. SVM is among the best performers for diverse classification tasks and is robust and scalable. Random forests are easy to train and tune, and they are preferable to pure bagging ensembles because of the reduction in collinearity between the bootstrapped trees. Naïve Bayes is robust to noise and irrelevant attributes, and only requires a single scan of the data, which is important for us since our data set is relatively large. Logistic regression utilizes parameters with useful interpretations and is quite robust and well-developed. In addition, in our preliminary research for the project, we found that other groups had attempted similar basketball classification problems using logistic regression. Gradient boosted decision trees are accurate, robust, and relatively interpretable. Next, XGBoost is portable and distributed, and has been the algorithm of choice for many winning teams in machine learning competitions. Finally, since all of these models have implementations in the Scikit-Learn library in Python, we found that these were easy to utilize and obtain reliable results with.
 
After selecting which models we would use to classify, we had to go about tuning the various parameters for each model and splitting the data into training and test sets. For SVM, we did not end up using any cross-validation after some testing. The model was taking too long to finish fitting the data with cross-validation, so we decided to remove the cross-validation completely. For the model, we used the radial kernel and a slack penalty of one. In our random forest classifier, we used cross-validation to split the data into training and test sets and to evaluate our classification rates, with 10 splits and a random state of 7. We used a similar method of cross-validation for Naïve Bayes, gradient boosted decision trees, and XGBoost. For the gradient boosted decision trees, we settled on using 100 estimators, a learning rate of 0.1, and the mean squared error with the Friedman improvement score for the split criterion. With the XGBoost model, we used a max depth of 3, a learning rate of 0.1, and 100 estimators. Finally, we employed the GridSearchCV cross-validation library to tune values of the regularization constant in our logistic regression model. In addition, we used the train_test_split function to split our data into training and test sets.
 
We did some preliminary fitting and testing on our data set, and the classification rates for the majority of our models were not much higher than the baseline classification rate of 53%. After examining our data and using our knowledge of the general distribution of shots taken in the NBA, we decided to partition our data set based on 2- and 3-point shots, and implement models for both of these partitions. We hypothesized that shot distance would be an important feature in classifying, and by splitting the data into 2- and 3-point shot attempts, we would have less variance in the shot distance in each set, compared to the entire data set. Additionally, with the analytics movement in the NBA today, teams have moved towards attempting more layups and three-point shots, and cutting down midrange attempts. Therefore, if we can better predict closer 2-point shots and 3-point shots, while sacrificing some accuracy in the midrange area, we can develop a model that will be more in tune with the changes currently affecting basketball.
 
Each of the models that we selected provides us with some unique information about our data set and our results. The SVM model will predict results based on the shots closest to the 2-pt/3-pt decision boundary. With the random forest model, gradient boosted decision trees, and XGBoost, we can obtain feature importance results. With these, we can show which features in the data set contributed most to the predictions made by the respective models. The Naïve Bayes model assumes that each shot is independent given its class (shot made/shot missed), and we can test this independence assumption by examining the results from the model. Lastly, the logistic regression model can tell us how unit changes in a single feature affects the log odds of predicting a shot as going in.

## Results
Shown below are the ROC curves for the unpartitioned data set for the gradient boosted decision trees, XGBoost, and SVM. As seen, gradient boosted decision trees and XGBoost performed similarly in classifying, as they had nearly equal area-under-the-curve (AUC) scores. In contrast, SVM performed much worse, possibly due to an ineffective kernel function, or the fact that SVM was so computationally intensive that we were unable to tune parameters.

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/GBDT_all.png "GBDT ROC - all shots")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/XGB_all.png "XGB ROC - all shots")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/SVM_all.png "SVM ROC - all shots")

After observing the bimodal distribution of our shot distances, we split our data into 2-point and 3-point shot attempts to see if we could create more accurate models for each our partition. As represented in the ROC curves in the figures below, the split did not result in more accurate models.

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/GBDT_2vs3.png "GDBT ROC - 2 vs 3")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/XGB_2vs3.png "XGB ROC - 2 vs 3")

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/SVM_2vs3.png "SVM ROC - 2 vs 3")

To truly be able to interpret our results, we needed to identify a baseline classification rate so that we would compare our results to that baseline. In our data set, 47% of the shots attempted were made. Therefore, if we created a simple model that guessed that every shot would miss, we would have a classification rate of 53% for our data set. We utilized this baseline to evaluate how well our models performed.

The classification rates for each model, on the entire data set and the partitioned data, is shown below.

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/ClassificationRates.PNG "Classification rates")

The best classification rate we were able to achieve was only 62% for the unpartitioned shot data compared to our baseline 53% accuracy. After splitting our data into 2-point shots and 3-point shots, we only achieved a 61% accuracy and 66% accuracy respectively. An important aspect to keep in mind about the higher accuracy for 3-point shots is that our baseline changes. The average 3-point field goal percentage of all the shots in our data is 33%. If we guessed that every shot would result in a miss, we would have an accuracy of around 66.7%. We can conclude that splitting our data into different types of shots did not do much better than the baseline. 

## Lessons and Findings
Our initial intuition for the most important features turned out to be true. Our best classifier, XGBoost, indicated that shot distance and closest defender distance determined the likelihood of a shot better than any other features. The feature importance figure below shows that a shooter has the highest chance of making a shot if he is close to the basket and a defender is far away. It also shows that the quarter had minimal effect on predicting field goals made which would mean shooters don’t shoot differently from quarter to quarter on average. However, this might not stay true if we consider each team separately. Some well-coached teams are known to make good adjustments at the halftime break after realizing the opponents tactics for the game.  In fact, such teams like the Golden State Warriors and the San Antonio Spurs outscored opponents by a combined 477 and 249 points in the 2016 NBA season respectively, higher than any other quarter. When considering all players and shots for a season, our model illustrates that one quarter is not necessarily better than another for shooting. 

![alt text](https://github.com/varunsridhar1/NBAShotPredictor/blob/master/Images/XGB_FeatureImportance.png "XGB feature importance")

We also gained some unexpected insights through this project such as the how important the shot clock value is, how unimportant location of the game is, and how splitting the data into 2-point and 3-point shots did not have a significant impact. The figure above tells us shots taken early in the shot clock have a higher chance of going in than others. To be clear, this does not suggest that players should start shooting the ball as quickly as possible but rather describes a certain phenomenon about shots. Multiple factors can cause a player to shoot quickly. Fast breaks and cross-court passes result in a shot relatively fast. Coaches can use timeouts to bring an inbound pass closer to the hoop where they can run a play. With more information about the events before the shot was taken such as the passer, type of shot, and assist percentages, we can further assess shot quality. Furthermore, we can see that location of the game does not have a strong impact on a shot. Though teams perform better when playing in their home location, our model indicates that the location does not affect the probability of a made shot. Finally, partitioning our data set into the two types of shots resulted in classification accuracies near the baseline. One of the reasons we suspect this is because the partitioned data set had less variance than the unpartitioned data set. We think the overall classification accuracy will increase with more season long features for players which we outline in our further research. 


